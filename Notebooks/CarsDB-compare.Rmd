---
jupyter:
  jupytext:
    formats: ipynb,Rmd
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.1'
      jupytext_version: 1.1.7-rc0
  kernelspec:
    display_name: Python 3
    language: python
    name: python3
---

# Compare results in the DB

```{python}
## To autoreload codein python files here.
# %load_ext autoreload
# %autoreload 2

## Add %%black at the top of a cell, and re-evaluate it, 
# to format it before git-commits, and ease diffs.
# %load_ext blackcellmagic
```

```{python}
from typing import Union, List, Callable, Any, Sequence as Seq
import io
import logging
from pathlib import PurePosixPath as P


from columnize import columnize
import numpy as np
import pandas as pd
from pandas import HDFStore
from pandas.core.generic import NDFrame
from matplotlib import pyplot as plt
import qgrid
from wltp.experiment import Experiment

import nbutils as nbu

idx = pd.IndexSlice
log = logging.getLogger('CarsDB-compare.ipynb')
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s|%(levelname)4.4s|%(module)s:[%(funcName)s]:\n  +--> %(message)s', 
    datefmt='%Y-%m-%d,%H:%M:%S',
)
```

```{python}
## DEFINITIONS
#
inp_h5fname = 'VehData/WltpGS-msaccess.h5'
out_h5fname = 'VehData/WltpGS-python.h5'
c_n, c_p, c_n_norm, c_p_norm = 'n', 'Pwot', 'n_norm', 'p_norm'
```

```{python}
nbu.print_nodes(inp_h5fname)
nbu.print_nodes(out_h5fname)
```

```{python}
def load_heinz_and_python_datasets(veh_nums=None):
    ip1, p1, c1 = nbu.merge_db_vehicle_subgroups(inp_h5fname, ("iprop", "oprop", "cycle"), veh_nums)
    p2, c2 = nbu.merge_db_vehicle_subgroups(out_h5fname, ("oprop", "cycle"), veh_nums)

    ## Merge input & Output props from heinz db.
    #
    ip1.name, p1.name = "ab"  # da butare
    p1 = pd.concat((ip1, p1), axis=0).sort_index()
    p1 = p1[~p1.index.duplicated()]  # or else, pivot(unstack) fails below

    ## By the spec, V rounded to 2-digits,
    #  But exporting MSAccess --> Excel outputs garbage decimals!
    #
    v_cols = "v v_orig v_cap v_downscale".split()
    c1[v_cols] = c1[v_cols].round(1)

    return p1, c1, p2, c2

p1, c1, p2, c2 = load_heinz_and_python_datasets()
```

```{python}
list(p1.index.levels[1])
```

```{python}
display(
    nbu.grid(p1),
    nbu.grid(p2),
    #nbu.grid(c1, fitcols=0),
)
```

```{python}
sr_cmpr = nbu.Comparator(lambda d, c: d[:, c], no_styling=True)
dataset_names = "Heinz Python".split()
```

```{python}
## Report PROP differences
#
equivalent_columns = [
    ("Description", None),
    ("test_mass", None),
    ("kerb_mass", None),
    ("class", "wltc_class"),
    ("pmr_km", "pmr"),
    ("f_dsc_req", "f_downscale"),
    ("v_max", "v_max")
]

cdf = sr_cmpr.compare((p1, p2), equivalent_columns, dataset_names)
## Workaround qgrid's hate for hierarchical-columns:
#  https://github.com/quantopian/qgrid/issues/18#issuecomment-149321165
cdf.columns = [' '.join(col).strip() for col in cdf.columns.values]

display(nbu.grid(cdf, fitcols=False, cwidth=100),)
```

```{python}
print(columnize(list(c1.columns), displaywidth=160))
print(c2.columns)
```

```{python}
cmpr = nbu.Comparator(lambda d, c: d.loc[idx[:, c]])
```

```{python}
## Report CYCLE-MEAN differences
#
# # Vehicles with DOWNSCALE discrepancies
# veh_nums = [7, 19, 20, 33, 35, 43, 44, 56, 59, 66]
# veh_nums += [82, 88, 91, 99, 100, 101, 112, 113, 114]
# # UNCOMMENT next line to FETCH new vehicles.
p1, c1, p2, c2 = load_heinz_and_python_datasets()
equivalent_series = [
    ("v_orig", "v_class"),
    ("v_downscale", "v_target"),
    ("a", "a_target"),
    ("P_tot", "p_required"),
    ("g_max", "gears_orig"),
    ("gear", "gears"),
    ("P_max", "p_available"),
    ("nc", "rpm"),
]
cols1, cols2 = zip(*equivalent_series)
cols1, cols2 = list(cols1), list(cols2)
cc1 = pd.concat((p1.unstack(), c1[cols1].abs().mean(level=0)), axis=1)
cc2 = pd.concat((p2.unstack(), c2[cols2].abs().mean(level=0)), axis=1)

## Convert props to numerics
cc1[cols1 + ["pmr_km", "f_dsc_req"]] = cc1[cols1 + ["pmr_km", "f_dsc_req"]].astype(
    "float64"
)
cc2[cols2 + ["f_downscale"]] = cc2[cols2 + ["f_downscale"]].astype("float64")

equivalent_props = [
    ("Description", None),
    ("class", None),
    ("pmr_km", None),
    ("f_dsc_req", "f_downscale"),
]
display(cmpr.compare((cc1, cc2), equivalent_props + equivalent_series, dataset_names))
```

```{python}
import wltp.plots as wp


def plot_xy_diffs_arrows(
    df: pd.DataFrame,
    cols_x1y1x2y2: Seq[str],
    data_label,
    ref_label=None,
    data_fmt="+k",
    data_kws={},
    diff_label=None,
    diff_fmt="-r",
    diff_cmap=wp.cm.hsv,
    diff_kws={},  # @UndefinedVariable cm.PiYG
    title=None,
    x_label=None,
    y_label=None,
    axes_tuple=None,
    mark_sections=None,
    distance_prcnt_threshold=1,
):
    # fix data: np.sqrt() works only on float64, and
    # indexing duped columns fetches df (not series).
    df = df[set(cols_x1y1x2y2)].astype(np.float64)
    X_REF, Y_REF, X, Y = [df[i] for i in cols_x1y1x2y2]

    df["Xdiff"] = X - X_REF
    df["Ydiff"] = Y - Y_REF

    # Filter out small differences.
    #
    df["dist_x"] = df["Xdiff"].abs() / X_REF
    df["dist_y"] = df["Ydiff"].abs() / Y_REF
    df["dist_max[%]"] = df[["dist_x", "dist_y"]].max(axis=1) * 100
    df = df.loc[df["dist_max[%]"] > distance_prcnt_threshold, :]

    X_REF, Y_REF, X, Y = [df[i] for i in cols_x1y1x2y2]
    U = df["Xdiff"]
    V = df["Ydiff"]

    ANGLE = phi = np.arctan2(V, U)

    color_diff = "r"
    alpha = 0.9
    cm_norm = wp.MidPointNorm()

    if axes_tuple:
        (axes, twin_axis) = axes_tuple
    else:
        bottom = 0.1
        height = 0.8
        axes = plt.axes([0.1, bottom, 0.80, height])

        ## Prepare axes
        #
        axes.set_xlabel(x_label)
        axes.set_ylabel(r"$%s$" % y_label.replace("$", ""))
        axes.xaxis.grid(True)
        axes.yaxis.grid(True)

        twin_axis = axes.twinx()
        twin_axis.set_ylabel(
            r"$\Delta %s$" % y_label.replace("$", ""), color=color_diff, labelpad=0
        )
        twin_axis.tick_params(axis="y", colors=color_diff)
        twin_axis.yaxis.grid(True, color=color_diff)

        axes.set_title(title)
        axes_tuple = (axes, twin_axis)

    if mark_sections == "classes":
        plot_class_limits(axes, Y.min())
    elif mark_sections == "parts":
        raise NotImplementedError()

    ## Plot data
    #
    l_ref = axes.quiver(
        X,
        Y,
        U,
        V,
        ANGLE,
        cmap=diff_cmap,
        norm=cm_norm,
        scale_units="xy",
        angles="xy",
        scale=1,
        width=0.004,
        alpha=alpha,
        pivot="tip",
    )

    l_data, = axes.plot(X, Y, data_fmt, label=data_label, **data_kws)
    l_data.set_picker(3)

    l_diff = twin_axis.plot(X, V, "o", color=color_diff, markersize=0.7)
    line_points, regress_poly = wp.fit_straight_line(X, V)
    l_diff_fitted, = twin_axis.plot(
        line_points,
        wp.polyval(regress_poly, line_points),
        diff_fmt,
        label=diff_label,
        **diff_kws
    )

    for name, x, y in zip(Y.index, X_REF, Y_REF):
        axes.annotate(name, (x, y), xycoords="data", size="xx-small")

    return axes_tuple, (l_data, l_ref, l_diff, l_diff_fitted)
```

```{python}
# %matplotlib inline

plt.close(); plt.figure()
plot_xy_diffs_arrows(pd.concat((cc1, cc2), axis=1), cols_x1y1x2y2=['pmr_km', "nc", 'pmr', 'rpm'], 
                     data_label="n in min-1", ref_label="HeinzDb",
#             data_fmt="+k", data_kws={},
#             diff_label=None, diff_fmt="-r", diff_cmap=cm.hsv, diff_kws={}, #@UndefinedVariable cm.PiYG
#             title='N', 
                     x_label='PMR', 
                     y_label='N_mean',
#             axes_tuple=None,
#             mark_sections=None
                    )
```

```{python}
from scipy.stats import pearsonr

def get_similarity(a, b):
    return pearsonr(a, b), a.abs().mean(), b.abs().mean()

#get_similarityc1.loc[veh, col1], c2.loc[veh, col2])
```

```{python}
# %matplotlib widget

_fig, (ax1, ax2) = plt.subplots(2, sharex=True, figsize=(12,8))

veh, col1, col2 = 'v001 P_tot p_required'.split()
tol = 0.00001
a, b = c1.loc[veh, col1].copy(), c2.loc[veh, col2].copy()
ix = 100*(a-b).abs()/a.max() > tol 
display('P', get_similarity(a, b))
a[ix].plot(ax=ax1)
b[ix].plot(ax=ax1)

ax1.grid(which='both')
ax1.legend()

#ax2 = ax.twinx()

col1, col2 = 'a a_target'.split()
a, b = c1.loc[veh, col1].copy(), c2.loc[veh, col2].copy()
ix = (a-b).abs()/a.max() > 0.8e-6
display(get_similarity(a, b))
#c1.loc[veh, 'v_orig'].copy()[ix].plot(style='3', ax=ax2)
a[ix].plot(style='+', ax=ax2)
b[ix].plot(style='x', ax=ax2)

ax2.grid(which='both')
ax2.legend()
```

```{python}
# %matplotlib widget

_fig, (ax1, ax2) = plt.subplots(2, sharex=True, figsize=(12,8))

veh, col1, col2 = 'v056 P_tot p_required'.split()
tol = 0.00001
a, b = c1.loc[veh, col1].copy(), c2.loc[veh, col2].copy()
ix = 100*(a-b).abs()/a.max() > tol 
display('P', get_similarity(a, b))
a[ix].plot(ax=ax1)
b[ix].plot(ax=ax1)

ax1.grid(which='both')
ax1.legend()

#ax2 = ax.twinx()

col1, col2 = 'v_downscale v_target'.split()
a, b = c1.loc[veh, col1].copy(), c2.loc[veh, col2].copy()
ix = a != b
display(get_similarity(a, b))
c1.loc[veh, 'v_orig'].copy()[ix].plot(style='3', ax=ax2)
a[ix].plot(style='+', ax=ax2)
b[ix].plot(style='x', ax=ax2)

ax2.grid(which='both')
ax2.legend()
```

```{python}
 c1.loc['v001', :].shape
```

```{python}

```
