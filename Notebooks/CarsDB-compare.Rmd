---
jupyter:
  jupytext:
    formats: ipynb,Rmd
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.1'
      jupytext_version: 1.2.1
  kernelspec:
    display_name: Python 3
    language: python
    name: python3
---

# Compare results in the DB

```{python}
## To autoreload codein python files here.
# %load_ext autoreload
# %autoreload 2

## Add %%black at the top of a cell, and re-evaluate it, 
# to format it before git-commits, and ease diffs.
# %load_ext blackcellmagic
```

```{python}
from typing import Union, List, Callable, Any, Sequence as Seq
import io
import logging
import sys
from pathlib import Path, PurePosixPath as P


from columnize import columnize
import numpy as np
import pandas as pd
from pandas import HDFStore
from pandas.core.generic import NDFrame
from matplotlib import pyplot as plt
import qgrid
import wltp
from wltp.experiment import Experiment

## Add tests/ into `sys.path` to import `vehdb` module.
#
proj_dir = str(Path(wltp.__file__).parents[1] / "tests")
if proj_dir not in sys.path:
    sys.path.insert(0, proj_dir)

import vehdb

idx = pd.IndexSlice
log = logging.getLogger('CarsDB-compare.ipynb')
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s|%(levelname)4.4s|%(module)s:[%(funcName)s]:\n  +--> %(message)s', 
    datefmt='%Y-%m-%d,%H:%M:%S',
)
```

```{python}
## DEFINITIONS
#
inp_h5fname = 'VehData/WltpGS-msaccess.h5'
out_h5fname = 'VehData/WltpGS-pyalgo.h5'
c_n, c_p, c_n_norm, c_p_norm = 'n', 'Pwot', 'n_norm', 'p_norm'
```

```{python}
vehdb.print_nodes(inp_h5fname)
vehdb.print_nodes(out_h5fname)
```

```{python}
def load_heinz_and_python_datasets(veh_nums=None):
    p1, c1 = vehdb.merge_db_vehicle_subgroups(inp_h5fname, ("prop", "cycle"), veh_nums)
    p2, c2 = vehdb.merge_db_vehicle_subgroups(out_h5fname, ("oprop", "cycle"), veh_nums)

    ## By the spec, V rounded to 2-digits,
    #  But exporting MSAccess --> Excel outputs garbage decimals!
    #
    v_cols = "v v_orig v_cap v_downscale".split()
    c1[v_cols] = c1[v_cols].round(1)

    return p1, c1, p2, c2

p1, c1, p2, c2 = load_heinz_and_python_datasets()
```

```{python}
print(columnize(list(p1.columns), displaywidth=160))
print(columnize(list(p2.columns), displaywidth=160))
```

```{python}
display(
    vehdb.grid(p1),
    vehdb.grid(p2),
    #vehdb.grid(c1, fitcols=0),
)
```

```{python}
sr_cmpr = vehdb.Comparator(lambda d, c: d[c], no_styling=True)
dataset_names = "accdb Python".split()
```

```{python}
## Report PROP differences
#
#     ACCDB,  PYALGO
equivalent_columns = [
    ("Description", None),
    ("test_mass", None),
    ("kerb_mass", None),
    ("vehicle_class", "wltc_class"),
    #("pmr_km", "pmr"),
    ("f_dsc_req", "f_downscale"),
    ("v_max", "v_max"),
    ("n_max1", "n95_high"),
]

cdf = sr_cmpr.compare((p1, p2), equivalent_columns, dataset_names)
## Workaround qgrid's hate for hierarchical-columns:
#  https://github.com/quantopian/qgrid/issues/18#issuecomment-149321165
cdf.columns = [' '.join(col).strip() for col in cdf.columns.values]

display(vehdb.grid(cdf, fitcols=False, cwidth=100),)
```

```{python}
print(columnize(list(c1.columns), displaywidth=160))
print(p2.columns, c2.columns)
```

```{python}
# %pdb off
```

```{python}
cmpr = vehdb.Comparator(lambda d, c: d[c])
```

```{python}
## Report CYCLE-MEAN differences
#
# # Vehicles with DOWNSCALE discrepancies
# veh_nums = [7, 19, 20, 33, 35, 43, 44, 56, 59, 66]
# veh_nums += [82, 88, 91, 99, 100, 101, 112, 113, 114]
# # UNCOMMENT next line to FETCH new vehicles.
p1, c1, p2, c2 = load_heinz_and_python_datasets()
equivalent_series = [
    ("v_orig", "v_class"),
    ("v_downscale", "v_target"),
    ("a", "a_target"),
    ("P_tot", "p_required"),
    ("g_max", "gears_orig"),
    ("gear", "gears"),
    ("P_max", "p_available"),
    ("nc", "rpm"),
]
cols1, cols2 = zip(*equivalent_series)
cols1, cols2 = list(cols1), list(cols2)
cc1 = pd.concat((p1, c1[cols1].abs().mean(level=0)), axis=1)
cc2 = pd.concat((p2, c2[cols2].abs().mean(level=0)), axis=1)

## Convert props to numerics
cc1[cols1 + ["pmr_km", "f_dsc_req"]] = cc1[cols1 + ["pmr_km", "f_dsc_req"]].astype(
    "float64"
)
cc2[cols2 + ["f_downscale"]] = cc2[cols2 + ["f_downscale"]].astype("float64")

equivalent_props = [
    ("Description", None),
    ("vehicle_class", None),
    ("pmr_km", None),
#     ("f_dsc_req", "f_downscale"),
    ("n_max1", "n95_high"),
    ("n_max2", "n_max2"),
    ("n_max3", "n_max3"),
    ("n_vmax", "n_max"),
#     ("n_max_wot", "n_max"),
]
display(cmpr.compare((cc1, cc2), equivalent_props + equivalent_series, dataset_names, describe=True))
```

```{python}
cmpr = vehdb.Comparator(lambda d, c: d[c])
display(cmpr.compare((cc1, cc2), equivalent_props + equivalent_series, dataset_names, no_styling=True))
```

```{python}
import wltp.plots as wp


def plot_xy_diffs_arrows(
    df: pd.DataFrame,
    cols_x1y1x2y2: Seq[str],
    data_label,
    ref_label=None,
    data_fmt="+k",
    data_kws={},
    diff_label=None,
    diff_fmt="-r",
    diff_cmap=wp.cm.hsv,
    diff_kws={},  # @UndefinedVariable cm.PiYG
    title=None,
    x_label=None,
    y_label=None,
    axes_tuple=None,
    mark_sections=None,
    distance_prcnt_threshold=1,
):
    # fix data: np.sqrt() works only on float64, and
    # indexing duped columns fetches df (not series).
    df = df[set(cols_x1y1x2y2)].astype(np.float64)
    X_REF, Y_REF, X, Y = [df[i] for i in cols_x1y1x2y2]

    df["Xdiff"] = X - X_REF
    df["Ydiff"] = Y - Y_REF

    # Filter out small differences.
    #
    df["dist_x"] = df["Xdiff"].abs() / X_REF
    df["dist_y"] = df["Ydiff"].abs() / Y_REF
    df["dist_max[%]"] = df[["dist_x", "dist_y"]].max(axis=1) * 100
    df = df.loc[df["dist_max[%]"] > distance_prcnt_threshold, :]

    X_REF, Y_REF, X, Y = [df[i] for i in cols_x1y1x2y2]
    U = df["Xdiff"]
    V = df["Ydiff"]

    ANGLE = phi = np.arctan2(V, U)

    color_diff = "r"
    alpha = 0.9
    cm_norm = wp.MidPointNorm()

    if axes_tuple:
        (axes, twin_axis) = axes_tuple
    else:
        bottom = 0.1
        height = 0.8
        axes = plt.axes([0.1, bottom, 0.80, height])

        ## Prepare axes
        #
        axes.set_xlabel(x_label)
        axes.set_ylabel(r"$%s$" % y_label.replace("$", ""))
        axes.xaxis.grid(True)
        axes.yaxis.grid(True)

        twin_axis = axes.twinx()
        twin_axis.set_ylabel(
            r"$\Delta %s$" % y_label.replace("$", ""), color=color_diff, labelpad=0
        )
        twin_axis.tick_params(axis="y", colors=color_diff)
        twin_axis.yaxis.grid(True, color=color_diff)

        axes.set_title(title)
        axes_tuple = (axes, twin_axis)

    if mark_sections == "classes":
        plot_class_limits(axes, Y.min())
    elif mark_sections == "parts":
        raise NotImplementedError()

    ## Plot data
    #
    l_ref = axes.quiver(
        X,
        Y,
        U,
        V,
        ANGLE,
        cmap=diff_cmap,
        norm=cm_norm,
        scale_units="xy",
        angles="xy",
        scale=1,
        width=0.004,
        alpha=alpha,
        pivot="tip",
    )

    l_data, = axes.plot(X, Y, data_fmt, label=data_label, **data_kws)
    l_data.set_picker(3)

    l_diff = twin_axis.plot(X, V, "o", color=color_diff, markersize=0.7)
    line_points, regress_poly = wp.fit_straight_line(X, V)
    l_diff_fitted, = twin_axis.plot(
        line_points,
        wp.polyval(regress_poly, line_points),
        diff_fmt,
        label=diff_label,
        **diff_kws
    )

    for name, x, y in zip(Y.index, X_REF, Y_REF):
        axes.annotate(name, (x, y), xycoords="data", size="xx-small")

    return axes_tuple, (l_data, l_ref, l_diff, l_diff_fitted)
```

```{python}
# %matplotlib inline

plt.close(); plt.figure()
plot_xy_diffs_arrows(pd.concat((cc1, cc2), axis=1), cols_x1y1x2y2=['pmr_km', "nc", 'pmr', 'rpm'], 
                     data_label="n in min-1", ref_label="HeinzDb",
#             data_fmt="+k", data_kws={},
#             diff_label=None, diff_fmt="-r", diff_cmap=cm.hsv, diff_kws={}, #@UndefinedVariable cm.PiYG
#             title='N', 
                     x_label='PMR', 
                     y_label='N_mean',
#             axes_tuple=None,
#             mark_sections=None
                    )
```

```{python}
from scipy.stats import pearsonr

def get_similarity(a, b):
    return pearsonr(a, b), a.abs().mean(), b.abs().mean()

#get_similarityc1.loc[veh, col1], c2.loc[veh, col2])
```

```{python}
# %matplotlib widget

_fig, (ax1, ax2) = plt.subplots(2, sharex=True, figsize=(12,8))

veh, col1, col2 = 'v001 P_tot p_required'.split()
tol = 0.00001
a, b = c1.loc[veh, col1].copy(), c2.loc[veh, col2].copy()
ix = 100*(a-b).abs()/a.max() > tol 
display('P', get_similarity(a, b))
if not a[ix].empty:
    a[ix].plot(ax=ax1)
if not b[ix].empty:
    b[ix].plot(ax=ax1)

ax1.grid(which='both')
ax1.legend()

#ax2 = ax.twinx()

col1, col2 = 'a a_target'.split()
a, b = c1.loc[veh, col1].copy(), c2.loc[veh, col2].copy()
ix = (a-b).abs()/a.max() > 0.8e-6
display(get_similarity(a, b))
#c1.loc[veh, 'v_orig'].copy()[ix].plot(style='3', ax=ax2)
if not a[ix].empty:
    a[ix].plot(style='+', ax=ax2)
if not b[ix].empty:
    b[ix].plot(style='x', ax=ax2)

ax2.grid(which='both')
ax2.legend()
```
